{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cc70d6",
   "metadata": {},
   "source": [
    "# ETL & DATA CLEANING : Pr√©diction de la Gravit√© des Accidents Routiers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook effectue un processus complet de **nettoyage et pr√©paration des donn√©es (ETL)** √† partir des bases de donn√©es BAAC (Bulletin d'Analyse d'Accidents de la circulation). Disponible sur le site suivant: https://www.data.gouv.fr/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2024\n",
    "\n",
    "**Objectif final** : Produire un dataset clean, encod√© et pr√™t pour la mod√©lisation ML.\n",
    "\n",
    "**Sources de donn√©es** (2021-2024) :\n",
    "- **Caract√©ristiques (caract)** : Contexte de l'accident (jour, heure, m√©t√©o, etc.)\n",
    "- **Lieux (lieux)** : Localisation g√©ographique et type de route\n",
    "- **V√©hicules (vehicules)** : Infos sur le v√©hicule impliqu√©\n",
    "- **Usagers (usagers)** : **Table centrale (de fait)** - chaque ligne = une personne impliqu√©e\n",
    "\n",
    "**Structure du pipeline** :   \n",
    "\n",
    "* Fusion Relationnelle : Agr√©gation de 4 ans de donn√©es (2021-2024) et jointures horizontales centr√©es sur la table Usagers.\n",
    "\n",
    "* Nettoyage de Structure : Suppression des colonnes vides √† plus de 70 %, retrait des donn√©es de localisation trop fines (PR/PR1) et √©limination des doublons.\n",
    "\n",
    "* Pr√©vention du Leakage : Suppression des variables connues uniquement apr√®s l'accident pour garantir l'int√©grit√© pr√©dictive du mod√®le.\n",
    "\n",
    "* Feature Engineering : Cr√©ation de variables √† fort signal : √¢ge, moment de la journ√©e (nuit/jour), weekend et m√©t√©o d√©grad√©e.\n",
    "\n",
    "* Finalisation : Imputation des valeurs manquantes (m√©diane/mode) et encodage final (One-Hot Encoding) pour rendre le dataset exploitable par une IA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc4a10",
   "metadata": {},
   "source": [
    "# 1. Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094060a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3715b",
   "metadata": {},
   "source": [
    "# 2. Pr√©paration et agr√©gation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6685f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des r√©pertoires\n",
    "raw_path = \"../data/raw/\"\n",
    "processed_path = \"../data/processed/\"\n",
    "os.makedirs(processed_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097eb82f",
   "metadata": {},
   "source": [
    "## 2.1 Concat√©nation (empilement vertical des ann√©es)   \n",
    "Les donn√©es sont fragment√©es par ann√©e (2021-2024). Pour une analyse globale, il faut regrouper ces fichiers pour chaque th√©matique (Usagers, V√©hicules, etc.).   \n",
    "Strat√©gie : Nous cr√©ons une fonction qui parcourt les ann√©es, lit les fichiers correspondants et les \"empile\" verticalement. Cette s√©paration permet d'ajouter facilement de nouvelles ann√©es au projet sans modifier la logique de fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67051704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historique Usagers : 506886 lignes.\n"
     ]
    }
   ],
   "source": [
    "years = [2021, 2022, 2023, 2024]\n",
    "\n",
    "def stack_years(category):\n",
    "    dfs = []\n",
    "    for y in years:\n",
    "        file = os.path.join(raw_path, f\"{category}-{y}.csv\")\n",
    "        if os.path.exists(file):\n",
    "            dfs.append(pd.read_csv(file, sep=';', encoding='utf-8', low_memory=False))\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Centralisation\n",
    "usagers_all = stack_years(\"usagers\")\n",
    "veh_all     = stack_years(\"vehicules\")\n",
    "caract_all  = stack_years(\"caract\")\n",
    "lieux_all   = stack_years(\"lieux\")\n",
    "\n",
    "print(f\"Historique Usagers : {len(usagers_all)} lignes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5652e",
   "metadata": {},
   "source": [
    "## 2.2 Jointure relationnelle  \n",
    "Les donn√©es BAAC sont morcel√©es en quatre fichiers annuels (Caract√©ristiques, Lieux, V√©hicules, Usagers). Une fusion maladroite peut entra√Æner une perte de donn√©es ou une duplication artificielle des lignes.   \n",
    " *Strat√©gie* : La table Usagers est notre table de fait centrale car la variable cible grav. Nous effectuons une jointure \"Left Join\" pour conserver tous les usagers et leur associer les caract√©ristiques de l'accident et du v√©hicule. La table lieux est d√©-dupliqu√©e au pr√©alable pour garantir qu'un accident ne pointe que vers une seule localisation g√©ographique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6735807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fusionn√© : 506886 lignes.\n"
     ]
    }
   ],
   "source": [
    "lieux_unique = lieux_all.drop_duplicates(subset='Num_Acc')\n",
    "\n",
    "df = pd.merge(usagers_all, veh_all, on=['Num_Acc', 'id_vehicule'], how='left', suffixes=('', '_v'))\n",
    "df = pd.merge(df, caract_all, on='Num_Acc', how='left')\n",
    "df = pd.merge(df, lieux_unique, on='Num_Acc', how='left')\n",
    "\n",
    "print(f\"Dataset fusionn√© : {df.shape[0]} lignes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954595af",
   "metadata": {},
   "source": [
    "## 3. Nettoyage des donn√©es (Data Cleaning)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de68a6",
   "metadata": {},
   "source": [
    "### 3.1 Gestion des valeurs \"Non Renseign√©es\"\n",
    "La documentation indique que les valeurs -1, 0 (pour certaines colonnes) ou les points \".\" correspondent √† des donn√©es non renseign√©es. Pour que le mod√®le ne les consid√®re pas comme des nombres r√©els, il faut les convertir en NaN.   \n",
    "Certaines variables sont trop vides ou trop pr√©cises pour √™tre g√©n√©ralisables par une IA.   \n",
    "Strat√©gie :   \n",
    " 1. Seuil de 70% : Nous supprimons les colonnes comme v1 ou secu3 qui sont vides √† plus de 70%. Cela √©vite d'introduire du bruit par une imputation massive.   \n",
    " 2. Suppression de PR/PR1 : Ces marqueurs kilom√©triques sont trop sp√©cifiques. Les retirer force le mod√®le √† apprendre des facteurs globaux (vitesse, m√©t√©o) plut√¥t que des coordonn√©es pr√©cises, am√©liorant ainsi sa capacit√© de g√©n√©ralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bddc2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DES DONN√âES NON RENSEIGN√âES (AVANT NETTOYAGE)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nb valeurs \"vides\"</th>\n",
       "      <th>% du total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>v1</th>\n",
       "      <td>505991</td>\n",
       "      <td>99.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secu3</th>\n",
       "      <td>501513</td>\n",
       "      <td>98.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locp</th>\n",
       "      <td>467730</td>\n",
       "      <td>92.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etatp</th>\n",
       "      <td>467713</td>\n",
       "      <td>92.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vosp</th>\n",
       "      <td>447810</td>\n",
       "      <td>88.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infra</th>\n",
       "      <td>430410</td>\n",
       "      <td>84.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obs</th>\n",
       "      <td>424706</td>\n",
       "      <td>83.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secu2</th>\n",
       "      <td>404723</td>\n",
       "      <td>79.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actp</th>\n",
       "      <td>229965</td>\n",
       "      <td>45.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trajet</th>\n",
       "      <td>144900</td>\n",
       "      <td>28.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr1</th>\n",
       "      <td>115668</td>\n",
       "      <td>22.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>101544</td>\n",
       "      <td>20.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obsm</th>\n",
       "      <td>98318</td>\n",
       "      <td>19.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secu1</th>\n",
       "      <td>58043</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manv</th>\n",
       "      <td>32449</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Nb valeurs \"vides\"  % du total\n",
       "v1                  505991       99.82\n",
       "secu3               501513       98.94\n",
       "locp                467730       92.28\n",
       "etatp               467713       92.27\n",
       "vosp                447810       88.35\n",
       "infra               430410       84.91\n",
       "obs                 424706       83.79\n",
       "secu2               404723       79.84\n",
       "actp                229965       45.37\n",
       "trajet              144900       28.59\n",
       "pr1                 115668       22.82\n",
       "pr                  101544       20.03\n",
       "obsm                 98318       19.40\n",
       "secu1                58043       11.45\n",
       "manv                 32449        6.40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "APER√áU DE 5 LIGNES CONTENANT DES VALEURS -1 ou 0 :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_Acc</th>\n",
       "      <th>id_usager</th>\n",
       "      <th>id_vehicule</th>\n",
       "      <th>num_veh</th>\n",
       "      <th>place</th>\n",
       "      <th>catu</th>\n",
       "      <th>grav</th>\n",
       "      <th>sexe</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>trajet</th>\n",
       "      <th>...</th>\n",
       "      <th>prof</th>\n",
       "      <th>pr</th>\n",
       "      <th>pr1</th>\n",
       "      <th>plan</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>larrout</th>\n",
       "      <th>surf</th>\n",
       "      <th>infra</th>\n",
       "      <th>situ</th>\n",
       "      <th>vma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202100000001</td>\n",
       "      <td>267¬†638</td>\n",
       "      <td>201¬†764</td>\n",
       "      <td>B01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202100000001</td>\n",
       "      <td>267¬†639</td>\n",
       "      <td>201¬†765</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202100000002</td>\n",
       "      <td>267¬†636</td>\n",
       "      <td>201¬†762</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202100000002</td>\n",
       "      <td>267¬†637</td>\n",
       "      <td>201¬†763</td>\n",
       "      <td>B01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202100000003</td>\n",
       "      <td>267¬†634</td>\n",
       "      <td>201¬†761</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Num_Acc id_usager id_vehicule num_veh  place  catu  grav  sexe  \\\n",
       "0  202100000001   267¬†638     201¬†764     B01      1     1     3     1   \n",
       "1  202100000001   267¬†639     201¬†765     A01      1     1     1     1   \n",
       "2  202100000002   267¬†636     201¬†762     A01      1     1     4     1   \n",
       "3  202100000002   267¬†637     201¬†763     B01      1     1     3     1   \n",
       "4  202100000003   267¬†634     201¬†761     A01      1     1     1     1   \n",
       "\n",
       "   an_nais  trajet  ...  prof   pr  pr1  plan lartpc  larrout surf  infra  \\\n",
       "0   2000.0       1  ...     1  (1)  (1)     1    NaN       -1    1      0   \n",
       "1   1978.0       1  ...     1  (1)  (1)     1    NaN       -1    1      0   \n",
       "2   1983.0       0  ...     1    0   10     1    NaN       -1    1      0   \n",
       "3   1993.0       0  ...     1    0   10     1    NaN       -1    1      0   \n",
       "4   1995.0       1  ...     1  (1)  (1)     1    NaN       -1    1      0   \n",
       "\n",
       "   situ  vma  \n",
       "0     1   80  \n",
       "1     1   80  \n",
       "2     1   80  \n",
       "3     1   80  \n",
       "4     1   50  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspection avant nettoyage \n",
    "\n",
    "# 1. On d√©finit ce qu'on cherche\n",
    "valeurs_vides = [-1, 0, '-1', '0', '.', '0.0']\n",
    "\n",
    "# 2. On cr√©e un masque : True si la cellule contient une valeur \"vide\", False sinon\n",
    "masque_vides = df.isin(valeurs_vides)\n",
    "\n",
    "# 3. On compte par colonne (en ignorant Num_Acc et grav pour l'instant)\n",
    "compte_vides = masque_vides.drop(columns=['Num_Acc', 'grav']).sum().sort_values(ascending=False)\n",
    "\n",
    "# 4. On calcule le pourcentage\n",
    "pourcentage_vides = (compte_vides / len(df) * 100).round(2)\n",
    "\n",
    "# 5. On affiche un r√©sum√©\n",
    "print(\"ANALYSE DES DONN√âES NON RENSEIGN√âES (AVANT NETTOYAGE)\")\n",
    "print(\"-\" * 50)\n",
    "resume_vides = pd.DataFrame({\n",
    "    'Nb valeurs \"vides\"': compte_vides,\n",
    "    '% du total': pourcentage_vides\n",
    "})\n",
    "\n",
    "# On ne montre que les colonnes qui ont au moins une valeur vide\n",
    "display(resume_vides[resume_vides['Nb valeurs \"vides\"'] > 0].head(15))\n",
    "\n",
    "# 6. √âchantillon des lignes \"√† probl√®mes\"\n",
    "print(\"\\nAPER√áU DE 5 LIGNES CONTENANT DES VALEURS -1 ou 0 :\")\n",
    "# On cherche les lignes qui ont au moins un \"vide\" dans n'importe quelle colonne\n",
    "lignes_a_problemes = df[masque_vides.any(axis=1)]\n",
    "display(lignes_a_problemes.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b382510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des codes m√©tier (-1, 0, .) par NaN\n",
    "df = df.replace([-1, 0, \"-1\", \"0\", \".\", \"0.0\"], np.nan)\n",
    "\n",
    "# 1. Seuil de 70% de vides\n",
    "seuil = 0.70\n",
    "limit = len(df) * (1 - seuil)\n",
    "df = df.dropna(thresh=limit, axis=1)\n",
    "\n",
    "# 2. Suppression PR/PR1 (trop pr√©cis)\n",
    "df = df.drop(columns=['pr', 'pr1'], errors='ignore')\n",
    "\n",
    "# Nettoyage de la cible\n",
    "df = df.dropna(subset=['grav'])\n",
    "df['grav'] = df['grav'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545b250",
   "metadata": {},
   "source": [
    "## 3.2. Gestion du Leakage et Doublons\n",
    "Les doublons et les variables connues \"apr√®s coup\" (leakage) faussent les performances du mod√®le.   \n",
    "Strat√©gie : Nous supprimons les doublons sur le couple (Num_Acc, id_usager). Nous retirons aussi les variables post-accident (actp, etatp, locp, secu2) car elles ne sont pas disponibles pour pr√©dire un accident avant qu'il n'arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1028e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doublons\n",
    "df = df.drop_duplicates(subset=['Num_Acc', 'id_usager'])\n",
    "\n",
    "# Data Leakage Prevention\n",
    "leak_cols = ['actp', 'etatp', 'locp', 'secu2', 'secu3']\n",
    "df = df.drop(columns=[c for c in leak_cols if c in df.columns], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83739a5a",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering    \n",
    "Les donn√©es brutes (heure, ann√©e naissance) sont peu digestes. Les trous restants doivent √™tre combl√©s.   \n",
    "Strat√©gie (Cr√©ation de variables √† fort signal ):\n",
    "\n",
    "* √Çge (vuln√©rabilit√©).\n",
    "\n",
    "* Weekend (comportements de loisirs).\n",
    "\n",
    "* Is_work_trip (stress li√© au travail vs loisirs).\n",
    "\n",
    "* Meteo_degradee (conditions d'adh√©rence).\n",
    "\n",
    "* Moment_journee (visibilit√©)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7a7574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√©rification des nouvelles variables :\n",
      "- Trajets travail : 21.93%\n",
      "- Accidents weekend : 20.71%\n"
     ]
    }
   ],
   "source": [
    "# 1. √Çge : Refl√®te la vuln√©rabilit√© physique de l'usager\n",
    "df['age'] = 2024 - df['an_nais']\n",
    "\n",
    "# 2. Moment de la journ√©e : Influence de la visibilit√© et de la fatigue\n",
    "def get_moment_journee(val):\n",
    "    # 1. Gestion des valeurs manquantes (NaN)\n",
    "    if pd.isna(val):\n",
    "        return 'inconnu'\n",
    "    \n",
    "    try:\n",
    "        # 2. Extraction de l'heure\n",
    "        # On g√®re le format '12:30' (split sur :) ou '1230' (HHMM)\n",
    "        val_str = str(val).strip()\n",
    "        \n",
    "        if ':' in val_str:\n",
    "            h = int(val_str.split(':')[0])\n",
    "        elif len(val_str) >= 3:\n",
    "            # Cas HHMM (ex: 1230 -> 12)\n",
    "            h = int(val_str[:-2])\n",
    "        else:\n",
    "            # Cas heure simple\n",
    "            h = int(val_str)\n",
    "            \n",
    "        # 3. Logique Nuit (22h-6h) vs Jour\n",
    "        if h < 6 or h > 21:\n",
    "            return 'nuit'\n",
    "        return 'jour'\n",
    "        \n",
    "    except (ValueError, TypeError, IndexError):\n",
    "        # En cas de format de texte inattendu\n",
    "        return 'inconnu'\n",
    "\n",
    "# Application de la fonction\n",
    "df['moment_journee'] = df['hrmn'].apply(get_moment_journee)\n",
    "\n",
    "# V√©rification\n",
    "#print(df['moment_journee'].value_counts())\n",
    "\n",
    "# 3. Extraction du Weekend : Comportements de loisirs vs semaine\n",
    "df['temp_date'] = pd.to_datetime(df[['jour', 'mois', 'an']].rename(columns={'jour': 'day', 'mois': 'month', 'an': 'year'}), errors='coerce')\n",
    "df['is_weekend'] = df['temp_date'].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# 4. Is_work_trip : Stress li√© au travail vs Loisirs\n",
    "# Strat√©gie : On regroupe les modalit√©s 1 (Domicile-Travail) et 4 (Usage professionnel) \n",
    "# pour isoler le risque routier professionnel.\n",
    "def map_work_trip(x):\n",
    "    try:\n",
    "        x = int(x)\n",
    "        if x in [1, 4]: # 1: Domicile-Travail, 4: Professionnel\n",
    "            return 1\n",
    "        return 0 # Loisirs, courses, √©cole, etc.\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['is_work_trip'] = df['trajet'].apply(map_work_trip)\n",
    "\n",
    "# 5. M√©t√©o D√©grad√©e : Simplification du signal atmosph√©rique (atm)\n",
    "def map_meteo(x):\n",
    "    try:\n",
    "        x = int(x)\n",
    "        if x in [3, 4, 5]: # 3: Pluie forte, 4: Neige/Gr√™le, 5: Brouillard\n",
    "            return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['meteo_degradee'] = df['atm'].apply(map_meteo)\n",
    "\n",
    "print(\"V√©rification des nouvelles variables :\")\n",
    "print(f\"- Trajets travail : {df['is_work_trip'].mean():.2%}\")\n",
    "print(f\"- Accidents weekend : {df['is_weekend'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee59a0",
   "metadata": {},
   "source": [
    "# 5. Nettoyage avanc√© et export\n",
    "Les algorithmes de ML ne supportent pas les NaN restants ni les variables textuelles. \n",
    "Strat√©gie : \n",
    "1. Imputation : M√©diane pour l'√¢ge (robuste) et Mode pour les cat√©gories. \n",
    "2. Bruit : Suppression des identifiants techniques et des colonnes redondantes. \n",
    "3. Encodage : Utilisation de get_dummies (One-Hot Encoding) pour transformer les textes en chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d53a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppression de s√©curit√© : 'hrmn' (1440 valeurs uniques)\n",
      "Suppression de s√©curit√© : 'dep' (107 valeurs uniques)\n",
      "Suppression de s√©curit√© : 'com' (19211 valeurs uniques)\n",
      "Suppression de s√©curit√© : 'lat' (157530 valeurs uniques)\n",
      "Suppression de s√©curit√© : 'long' (158687 valeurs uniques)\n",
      "Suppression de s√©curit√© : 'voie' (38376 valeurs uniques)\n",
      "Suppression de s√©curit√© : 'larrout' (150 valeurs uniques)\n",
      "Succ√®s ! Dimensions finales : (506467, 60)\n",
      "üíæ Dataset sauvegard√© : ../data/processed/dataset_final.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Suppression des identifiants et colonnes toxiques\n",
    "to_drop_final = ['Num_Acc', 'id_usager', 'id_vehicule', 'num_veh', 'num_veh_v', 'adr', 'temp_date']\n",
    "df = df.drop(columns=[c for c in to_drop_final if c in df.columns], errors='ignore')\n",
    "\n",
    "# 2. Filtre de s√©curit√© automatique (Cardinalit√©)\n",
    "# Supprime toute colonne texte restante qui d√©passe 100 modalit√©s\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    n_unique = df[col].nunique()\n",
    "    if n_unique > 100:\n",
    "        print(f\"Suppression de s√©curit√© : '{col}' ({n_unique} valeurs uniques)\")\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "# 3. Encodage One-Hot\n",
    "# Transforme les variables qualitatives (ex: moment_journee) en nombres 0/1\n",
    "df_final = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# 4. Conversion float32 pour l'√©conomie de m√©moire\n",
    "# R√©duit le poids du fichier final sans perdre d'information pr√©dictive\n",
    "df_final = df_final.astype('float32')\n",
    "\n",
    "print(f\"Succ√®s ! Dimensions finales : {df_final.shape}\")\n",
    "\n",
    "\n",
    "# 5. EXPORTATION FINALE\n",
    "\n",
    "output_file = \"../data/processed/dataset_final.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"üíæ Dataset sauvegard√© : {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
